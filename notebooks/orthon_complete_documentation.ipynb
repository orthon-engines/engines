{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ORTHON: Behavioral Geometry Engine\n## Complete Documentation, Experiments & Results\n\n---\n\n**Package:** `orthon` v0.1.0  \n**Purpose:** Industrial signal topology analysis for predictive maintenance  \n**Created:** January 2026  \n**Last Updated:** January 21, 2026  \n\n---\n\n## Table of Contents\n\n1. [Package Overview](#1-package-overview)\n2. [Installation](#2-installation)\n3. [Architecture](#3-architecture)\n4. [Engine Reference](#4-engine-reference)\n5. [Data Pipeline](#5-data-pipeline)\n6. [Feature Engineering Module](#6-feature-engineering-module)\n7. [Rolling Window Statistics](#7-rolling-window-statistics)\n8. [Cluster Normalization](#8-cluster-normalization)\n9. [C-MAPSS RUL Prediction Experiments](#9-c-mapss-rul-prediction-experiments)\n10. [Best Model Configuration](#10-best-model-configuration) ⭐ UPDATED\n11. [Extended Windows Experiment](#11-extended-windows-experiment)\n12. [Feature Selection with XGBoost](#12-feature-selection-with-xgboost)\n13. [Ablation Study Results](#13-ablation-study-results)\n14. [Regime-Aware Feature Engineering](#14-regime-aware-feature-engineering)\n15. [Complete Code Reference](#15-complete-code-reference)\n16. [Experiment Log](#16-experiment-log) ⭐ UPDATED\n17. [Stacking Ensemble Experiment](#17-stacking-ensemble-experiment) ⭐ NEW\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Package Overview\n",
    "\n",
    "## What is Orthon?\n",
    "\n",
    "**Orthon** is a behavioral geometry engine for industrial signal topology analysis. It computes:\n",
    "\n",
    "- **Intrinsic properties** of individual signals (Hurst exponent, entropy, GARCH volatility)\n",
    "- **Relational structure** between signals (PCA, clustering, mutual information, MST)\n",
    "- **Temporal dynamics** of system evolution (Granger causality, DTW, DMD)\n",
    "- **Feature engineering** for ML (rolling windows, cluster normalization)\n",
    "\n",
    "## Design Philosophy\n",
    "\n",
    "```\n",
    "Core Principles:\n",
    "├── Pure Polars + Parquet architecture (no database)\n",
    "├── Academic research standards (no shortcuts or approximations)\n",
    "├── Explicit time handling (nothing inferred)\n",
    "├── No implicit execution (importing does nothing)\n",
    "├── Publication-grade quality (peer-reviewed algorithms)\n",
    "└── Heavy feature engineering for ML pipelines\n",
    "```\n",
    "\n",
    "## Validated Domains\n",
    "\n",
    "| Domain | Source | Use Case | Signals |\n",
    "|--------|--------|----------|--------|\n",
    "| **C-MAPSS** | NASA | Turbofan engine degradation | 21 sensors |\n",
    "| **FEMTO** | PHM Society | Bearing degradation | Vibration |\n",
    "| **Hydraulic** | UCI | Hydraulic system monitoring | 17 sensors |\n",
    "| **CWRU** | Case Western | Bearing fault classification | Vibration |\n",
    "| **TEP** | Tennessee Eastman | Chemical process faults | 52 variables |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Installation\n",
    "\n",
    "## From Source (Development)\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/prism-engines/diagnostics.git\n",
    "cd diagnostics\n",
    "\n",
    "# Install in development mode\n",
    "pip install -e .\n",
    "\n",
    "# Verify installation\n",
    "python -c \"import orthon; print(orthon.__version__)\"\n",
    "# Output: 0.1.0\n",
    "```\n",
    "\n",
    "## With ML Dependencies\n",
    "\n",
    "```bash\n",
    "# Install with optional ML frameworks\n",
    "pip install -e \".[ml]\"\n",
    "\n",
    "# This includes:\n",
    "# - xgboost>=2.0.0\n",
    "# - lightgbm>=4.0.0\n",
    "# - catboost>=1.2.0\n",
    "```\n",
    "\n",
    "## Full Installation (All Dependencies)\n",
    "\n",
    "```bash\n",
    "pip install -e \".[all]\"\n",
    "```\n",
    "\n",
    "## CLI Verification\n",
    "\n",
    "```bash\n",
    "# Check version\n",
    "orthon --version\n",
    "# Output: orthon 0.1.0\n",
    "\n",
    "# List all available engines\n",
    "orthon --list-engines\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Architecture\n",
    "\n",
    "## Package Structure\n",
    "\n",
    "```\n",
    "orthon/\n",
    "├── __init__.py           # Public API exports (clean interface)\n",
    "├── cli.py                # Command-line interface\n",
    "├── py.typed              # PEP 561 type hints marker\n",
    "│\n",
    "├── features/             # ⭐ NEW: Feature Engineering Module\n",
    "│   ├── __init__.py\n",
    "│   └── rolling_features.py   # Rolling stats + cluster normalization\n",
    "│\n",
    "└── _internal/            # Bundled core implementation\n",
    "    ├── core/             # Core types and utilities\n",
    "    │   ├── domain_clock.py   # DomainClock, DomainInfo\n",
    "    │   └── signals/          # Signal types\n",
    "    │\n",
    "    ├── config/           # YAML configurations\n",
    "    │   ├── domain.py\n",
    "    │   ├── windows.py\n",
    "    │   └── cascade.py\n",
    "    │\n",
    "    ├── db/               # Parquet I/O layer\n",
    "    │   ├── parquet_store.py  # Core file operations\n",
    "    │   ├── polars_io.py      # Polars-native I/O\n",
    "    │   └── query.py          # Query utilities\n",
    "    │\n",
    "    ├── engines/          # 33+ Computation Engines\n",
    "    │   ├── windowed/         # Vector engines (signal-level)\n",
    "    │   ├── geometry/         # Geometry engines (relational)\n",
    "    │   ├── state/            # State engines (temporal)\n",
    "    │   ├── laplace/          # Laplace transform\n",
    "    │   ├── spectral/         # Wavelet analysis\n",
    "    │   └── pointwise/        # Derivatives, Hilbert\n",
    "    │\n",
    "    ├── utils/            # Utility functions\n",
    "    │   ├── adaptive_windows.py\n",
    "    │   ├── memory.py\n",
    "    │   └── monitor.py\n",
    "    │\n",
    "    └── entry_points/     # CLI entry points\n",
    "        ├── fetch.py\n",
    "        ├── signal_vector.py\n",
    "        ├── geometry.py\n",
    "        ├── state.py\n",
    "        └── ml_train.py\n",
    "```\n",
    "\n",
    "## Data Flow Pipeline\n",
    "\n",
    "```\n",
    "Layer 0: OBSERVATIONS\n",
    "         Raw sensor data from industrial systems\n",
    "         Output: data/observations.parquet\n",
    "              ↓\n",
    "Layer 1: VECTOR\n",
    "         Raw → 51 behavioral metrics per signal\n",
    "         (Hurst, entropy, GARCH, wavelets, etc.)\n",
    "         Output: data/vector.parquet\n",
    "              ↓\n",
    "Layer 2: GEOMETRY\n",
    "         Vector signals → Structural geometry\n",
    "         (PCA, clustering, MST, coupling)\n",
    "         Output: data/geometry.parquet\n",
    "              ↓\n",
    "Layer 3: STATE\n",
    "         Geometry evolution → Temporal dynamics\n",
    "         (Granger, DTW, transfer entropy)\n",
    "         Output: data/state.parquet\n",
    "              ↓\n",
    "Layer 4: FEATURE ENGINEERING  ⭐ NEW\n",
    "         Cluster normalization + Rolling window stats\n",
    "         Output: Hundreds of engineered features\n",
    "              ↓\n",
    "Layer 5: ML ACCELERATOR\n",
    "         All layers → Denormalized features → Model\n",
    "         Output: ml_features.parquet, ml_model.pkl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Engine Reference\n",
    "\n",
    "## 4.1 Vector Engines (11 engines)\n",
    "\n",
    "Single-signal analysis computing intrinsic properties.\n",
    "\n",
    "| Engine | Function | Description | Metrics |\n",
    "|--------|----------|-------------|--------|\n",
    "| **hurst** | `compute_hurst()` | Long-range dependence | H exponent, R/S analysis |\n",
    "| **entropy** | `compute_entropy()` | Information content | Sample, spectral, permutation |\n",
    "| **garch** | `compute_garch()` | Volatility clustering | GARCH(1,1) parameters |\n",
    "| **wavelet** | `compute_wavelets()` | Multi-scale decomposition | 8 scale coefficients |\n",
    "| **spectral** | `compute_spectral()` | Frequency analysis | Power spectrum, peaks |\n",
    "| **rqa** | `compute_rqa()` | Recurrence quantification | DET, LAM, entropy |\n",
    "| **lyapunov** | `compute_lyapunov()` | Chaos indicator | Largest Lyapunov exponent |\n",
    "| **realized_vol** | `compute_realized_vol()` | Realized volatility | 13 vol/drawdown metrics |\n",
    "| **hilbert_amplitude** | `compute_hilbert_amplitude()` | Instantaneous amplitude | Envelope |\n",
    "| **hilbert_phase** | `compute_hilbert_phase()` | Instantaneous phase | Phase angle |\n",
    "| **hilbert_frequency** | `compute_hilbert_frequency()` | Instantaneous frequency | Frequency |\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "import orthon\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample signal\n",
    "signal = np.random.randn(1000)\n",
    "\n",
    "# Compute Hurst exponent\n",
    "hurst_metrics = orthon.compute_hurst(signal)\n",
    "print(f\"Hurst exponent: {hurst_metrics['hurst_exp']:.3f}\")\n",
    "# Output: Hurst exponent: 0.512\n",
    "\n",
    "# Compute entropy\n",
    "entropy_metrics = orthon.compute_entropy(signal)\n",
    "print(f\"Sample entropy: {entropy_metrics['sample_entropy']:.3f}\")\n",
    "\n",
    "# Compute GARCH volatility\n",
    "garch_metrics = orthon.compute_garch(signal)\n",
    "print(f\"GARCH alpha: {garch_metrics['garch_alpha']:.4f}\")\n",
    "```\n",
    "\n",
    "## 4.2 Geometry Engines (9 engines)\n",
    "\n",
    "Multi-signal relational structure analysis.\n",
    "\n",
    "| Engine | Class | Description | Output |\n",
    "|--------|-------|-------------|--------|\n",
    "| **pca** | `PCAEngine` | Principal components | Explained variance, loadings |\n",
    "| **distance** | `DistanceEngine` | Pairwise distances | Distance matrix |\n",
    "| **clustering** | `ClusteringEngine` | Signal groupings | Cluster assignments |\n",
    "| **mutual_information** | `MutualInformationEngine` | Information coupling | MI matrix |\n",
    "| **copula** | `CopulaEngine` | Dependency structure | Copula parameters |\n",
    "| **mst** | `MSTEngine` | Minimum spanning tree | Graph topology |\n",
    "| **lof** | `LOFEngine` | Local outlier factor | Anomaly scores |\n",
    "| **convex_hull** | `ConvexHullEngine` | Boundary geometry | Hull volume, vertices |\n",
    "| **barycenter** | `BarycenterEngine` | Geometric center | Weighted centroid |\n",
    "\n",
    "## 4.3 State Engines (7 engines)\n",
    "\n",
    "Temporal dynamics and causality analysis.\n",
    "\n",
    "| Engine | Class | Description | Output |\n",
    "|--------|-------|-------------|--------|\n",
    "| **granger** | `GrangerEngine` | Granger causality | Causality matrix, p-values |\n",
    "| **cointegration** | `CointegrationEngine` | Long-run equilibrium | Cointegration vectors |\n",
    "| **cross_correlation** | `CrossCorrelationEngine` | Lag correlations | CCF matrix |\n",
    "| **dtw** | `DTWEngine` | Dynamic time warping | Warping distance |\n",
    "| **dmd** | `DMDEngine` | Dynamic mode decomposition | Modes, eigenvalues |\n",
    "| **transfer_entropy** | `TransferEntropyEngine` | Information flow | TE matrix |\n",
    "| **coupled_inertia** | `CoupledInertiaEngine` | Coupled dynamics | Inertia tensor |\n",
    "\n",
    "## 4.4 Observation-Level Engines (3 engines)\n",
    "\n",
    "Discontinuity detection at point precision.\n",
    "\n",
    "| Engine | Function | Description |\n",
    "|--------|----------|-------------|\n",
    "| **break_detector** | `get_break_metrics()` | All discontinuities |\n",
    "| **heaviside** | `get_heaviside_metrics()` | Persistent level shifts |\n",
    "| **dirac** | `get_dirac_metrics()` | Transient shocks |\n",
    "\n",
    "## 4.5 Temporal Dynamics Engines (5 engines)\n",
    "\n",
    "| Engine | Class | Description |\n",
    "|--------|-------|-------------|\n",
    "| **energy_dynamics** | `EnergyDynamicsEngine` | System energy evolution |\n",
    "| **tension_dynamics** | `TensionDynamicsEngine` | Structural tension changes |\n",
    "| **phase_detector** | `PhaseDetectorEngine` | Operating phase identification |\n",
    "| **cohort_aggregator** | `CohortAggregatorEngine` | Group behavior aggregation |\n",
    "| **transfer_detector** | `TransferDetectorEngine` | Information transfer detection |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Pipeline\n",
    "\n",
    "## I/O Functions\n",
    "\n",
    "```python\n",
    "import orthon\n",
    "\n",
    "# Path management\n",
    "data_root = orthon.get_data_root()  # Returns Path to data directory\n",
    "obs_path = orthon.get_path(orthon.OBSERVATIONS)  # data/observations.parquet\n",
    "\n",
    "# File constants\n",
    "orthon.OBSERVATIONS  # \"observations\"\n",
    "orthon.VECTOR        # \"vector\"\n",
    "orthon.GEOMETRY      # \"geometry\"\n",
    "orthon.STATE         # \"state\"\n",
    "orthon.COHORTS       # \"cohorts\"\n",
    "\n",
    "# Read data\n",
    "df = orthon.read_parquet(\"data/observations.parquet\")\n",
    "\n",
    "# Write data (atomic - safe for concurrent access)\n",
    "orthon.write_parquet_atomic(df, \"data/vector.parquet\")\n",
    "\n",
    "# Upsert (update existing rows, insert new)\n",
    "orthon.upsert_parquet(df, \"data/vector.parquet\", key_cols=[\"signal_id\", \"timestamp\"])\n",
    "\n",
    "# Append new rows\n",
    "orthon.append_parquet(new_df, \"data/observations.parquet\")\n",
    "\n",
    "# Query utilities\n",
    "stats = orthon.table_stats(\"data/observations.parquet\")\n",
    "desc = orthon.describe_table(\"data/observations.parquet\")\n",
    "```\n",
    "\n",
    "## Parquet Files\n",
    "\n",
    "| File | Description | Key Columns |\n",
    "|------|-------------|-------------|\n",
    "| `observations.parquet` | Raw sensor data | entity_id, timestamp, signal_id, value |\n",
    "| `vector.parquet` | Behavioral metrics | entity_id, timestamp, signal_id, + 51 metrics |\n",
    "| `geometry.parquet` | Structural snapshots | entity_id, timestamp, + geometry features |\n",
    "| `state.parquet` | Temporal dynamics | entity_id, timestamp, + state features |\n",
    "| `cohorts.parquet` | Entity groupings | entity_id, cohort_id, similarity |\n",
    "| `ml_features.parquet` | ML-ready features | entity_id, + all denormalized features |\n",
    "| `ml_model.pkl` | Trained model | Serialized sklearn/xgboost model |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature Engineering Module ⭐ NEW\n",
    "\n",
    "The `orthon.features` module provides heavy-duty feature engineering for time series ML.\n",
    "\n",
    "## Overview\n",
    "\n",
    "```python\n",
    "from orthon.features import (\n",
    "    # Configuration\n",
    "    RollingConfig,\n",
    "    \n",
    "    # Main engines\n",
    "    RollingFeatureEngine,     # Rolling window statistics\n",
    "    ClusterNormalizer,        # Cluster-based normalization\n",
    "    FeatureEngineeringPipeline,  # Combined pipeline\n",
    "    \n",
    "    # Convenience functions\n",
    "    compute_all_rolling_features,\n",
    "    compute_cluster_normalized_features,\n",
    ")\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "```python\n",
    "from orthon.features import FeatureEngineeringPipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = FeatureEngineeringPipeline(\n",
    "    n_clusters=6,                    # Operating regimes\n",
    "    windows=[5, 10, 20, 30, 50],     # Rolling window sizes\n",
    "    op_cols=['op_1', 'op_2'],        # Cluster by these\n",
    "    signal_cols=['s_11', 's_12'],    # Normalize these\n",
    "    healthy_pct=0.20,                # First 20% = healthy\n",
    ")\n",
    "\n",
    "# Fit on training data, transform both\n",
    "train_feat = pipeline.fit_transform(train_df, entity_col='unit', time_col='cycle')\n",
    "test_feat = pipeline.transform(test_df)\n",
    "\n",
    "# Result: 200+ engineered features\n",
    "print(f\"Features: {len(train_feat.columns)}\")\n",
    "```\n",
    "\n",
    "## What the Pipeline Does\n",
    "\n",
    "```\n",
    "Input: Raw sensor data with operating conditions\n",
    "           ↓\n",
    "Step 1: CLUSTER NORMALIZATION\n",
    "        ├── Cluster operating conditions (op_1, op_2) into regimes\n",
    "        ├── Compute healthy baselines per regime (first 20% of life)\n",
    "        └── Normalize signals as z-scores from regime baseline\n",
    "           ↓\n",
    "Step 2: ROLLING WINDOW STATISTICS\n",
    "        ├── Windows: [5, 10, 20, 30, 50] cycles\n",
    "        ├── Stats: mean, std, slope, delta, curvature, min, max, range, zscore, volatility\n",
    "        └── Applied to: hd_mean and top normalized signals\n",
    "           ↓\n",
    "Output: 200+ engineered features per observation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Rolling Window Statistics ⭐ NEW\n",
    "\n",
    "## 7.1 Configuration\n",
    "\n",
    "```python\n",
    "from orthon.features import RollingConfig, RollingFeatureEngine\n",
    "\n",
    "# Full configuration with all options\n",
    "config = RollingConfig(\n",
    "    # Window sizes (in cycles/timesteps)\n",
    "    windows=[5, 10, 20, 30, 50],\n",
    "    \n",
    "    # Basic statistics\n",
    "    compute_mean=True,        # Rolling mean\n",
    "    compute_std=True,         # Rolling standard deviation\n",
    "    compute_min=True,         # Rolling minimum\n",
    "    compute_max=True,         # Rolling maximum\n",
    "    compute_range=True,       # max - min\n",
    "    \n",
    "    # Trend features\n",
    "    compute_slope=True,       # Linear trend (degradation rate)\n",
    "    compute_delta=True,       # Change from N cycles ago\n",
    "    compute_curvature=True,   # Acceleration (slope of slope)\n",
    "    compute_momentum=True,    # Rate of change (1-step diff)\n",
    "    \n",
    "    # Distribution features\n",
    "    compute_skew=True,        # Distribution asymmetry\n",
    "    compute_kurtosis=True,    # Tail heaviness\n",
    "    compute_quantiles=True,   # q25, q50, q75\n",
    "    compute_iqr=True,         # Interquartile range\n",
    "    compute_cv=True,          # Coefficient of variation\n",
    "    \n",
    "    # Advanced features\n",
    "    compute_zscore=True,      # Current value as z-score of window\n",
    "    compute_volatility=True,  # Std of returns within window\n",
    "    compute_autocorr=True,    # Lag-1 autocorrelation\n",
    "    compute_entropy=False,    # Approximate entropy (slow)\n",
    ")\n",
    "```\n",
    "\n",
    "## 7.2 Feature Definitions\n",
    "\n",
    "| Feature | Formula | What It Captures |\n",
    "|---------|---------|------------------|\n",
    "| `mean_W` | `mean(window)` | Average level over window |\n",
    "| `std_W` | `std(window)` | Variability/volatility |\n",
    "| `min_W` | `min(window)` | Minimum value in window |\n",
    "| `max_W` | `max(window)` | Maximum value in window |\n",
    "| `range_W` | `max - min` | Spread of values |\n",
    "| `slope_W` | `polyfit(x, window, 1)[0]` | **Linear trend (degradation rate)** |\n",
    "| `delta_W` | `value[t] - value[t-W]` | **Change from W cycles ago** |\n",
    "| `curv_W` | `slope_2nd_half - slope_1st_half` | **Acceleration** |\n",
    "| `skew_W` | `scipy.stats.skew(window)` | Distribution asymmetry |\n",
    "| `kurt_W` | `scipy.stats.kurtosis(window)` | Tail heaviness |\n",
    "| `q25_W` | `percentile(window, 25)` | 25th percentile |\n",
    "| `q50_W` | `percentile(window, 50)` | Median |\n",
    "| `q75_W` | `percentile(window, 75)` | 75th percentile |\n",
    "| `iqr_W` | `q75 - q25` | Interquartile range |\n",
    "| `cv_W` | `std / abs(mean)` | Coefficient of variation |\n",
    "| `zscore_W` | `(value - mean) / std` | **Current position in window** |\n",
    "| `volatility_W` | `std(diff(window))` | Volatility of changes |\n",
    "| `autocorr_W` | `corrcoef(window[:-1], window[1:])` | Lag-1 autocorrelation |\n",
    "\n",
    "## 7.3 Usage Example\n",
    "\n",
    "```python\n",
    "from orthon.features import RollingFeatureEngine, RollingConfig\n",
    "import numpy as np\n",
    "\n",
    "# Create engine\n",
    "config = RollingConfig(windows=[10, 20, 30])\n",
    "engine = RollingFeatureEngine(config=config)\n",
    "\n",
    "# Single signal\n",
    "signal = np.random.randn(200)\n",
    "features = engine.compute(signal, prefix='sensor1')\n",
    "\n",
    "# Result: dict with feature arrays\n",
    "print(features.keys())\n",
    "# dict_keys(['sensor1_mean_10', 'sensor1_std_10', 'sensor1_slope_10', ...])\n",
    "\n",
    "# Multi-signal DataFrame\n",
    "df = engine.compute_multi_signal(\n",
    "    data=train_df,\n",
    "    signal_cols=['s_11', 's_12', 's_15'],\n",
    "    entity_col='unit',\n",
    "    sort_col='cycle',\n",
    ")\n",
    "```\n",
    "\n",
    "## 7.4 Why Different Window Sizes?\n",
    "\n",
    "| Window | Cycles | Captures |\n",
    "|--------|--------|----------|\n",
    "| **5** | Very short | Rapid changes, noise, sudden shifts |\n",
    "| **10** | Short | Short-term trends |\n",
    "| **20** | Medium | Medium-term dynamics |\n",
    "| **30** | Medium-long | Sustained trends |\n",
    "| **50** | Long | **Long-term degradation patterns** |\n",
    "\n",
    "**Key Finding**: Window 50 is crucial for RUL prediction. `hd_slope_50` captures sustained degradation trends and is consistently the #1 most important feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Cluster Normalization ⭐ NEW\n",
    "\n",
    "## 8.1 The Problem\n",
    "\n",
    "In multi-condition datasets (like C-MAPSS FD002), sensor values vary dramatically based on operating conditions:\n",
    "\n",
    "```\n",
    "Example: Sensor s11 (Total temperature at HPC outlet)\n",
    "\n",
    "Operating Condition 1: Healthy = 555°R, Degraded = 570°R\n",
    "Operating Condition 6: Healthy = 610°R, Degraded = 625°R\n",
    "\n",
    "WITHOUT normalization:\n",
    "  - 610°R from OC6 looks MORE degraded than 570°R from OC1\n",
    "  - Model learns WRONG signal!\n",
    "\n",
    "WITH cluster normalization:\n",
    "  - OC1: (570 - 555) / 5 = 3.0 std from healthy\n",
    "  - OC6: (625 - 610) / 5 = 3.0 std from healthy\n",
    "  - Same degradation level, correctly identified!\n",
    "```\n",
    "\n",
    "## 8.2 The Solution: Per-Regime Baselines\n",
    "\n",
    "```python\n",
    "from orthon.features import ClusterNormalizer\n",
    "\n",
    "normalizer = ClusterNormalizer(\n",
    "    n_clusters=6,           # Number of operating regimes\n",
    "    healthy_pct=0.20,       # First 20% of life = healthy\n",
    "    use_median=False,       # Use mean for baseline center\n",
    "    robust_std=True,        # Use MAD-based robust std\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "normalizer.fit(\n",
    "    data=train_df,\n",
    "    op_cols=['op_1', 'op_2'],        # Cluster by these\n",
    "    signal_cols=['s_11', 's_12'],    # Normalize these\n",
    "    entity_col='unit_id',\n",
    "    time_col='cycle',\n",
    ")\n",
    "\n",
    "# Transform with healthy distance features\n",
    "test_norm = normalizer.compute_healthy_distance(test_df)\n",
    "```\n",
    "\n",
    "## 8.3 What It Produces\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `cluster_id` | Assigned operating regime (0-5) |\n",
    "| `s_11_norm` | Z-score distance from regime baseline for s_11 |\n",
    "| `s_12_norm` | Z-score distance from regime baseline for s_12 |\n",
    "| `hd_mean` | **Mean healthy distance across all signals** |\n",
    "| `hd_max` | Maximum healthy distance (worst signal) |\n",
    "| `hd_std` | Standard deviation of healthy distances |\n",
    "| `hd_sum` | Sum of all healthy distances |\n",
    "| `hd_top1` | Highest healthy distance |\n",
    "| `hd_top2` | Second highest healthy distance |\n",
    "| `hd_top3` | Third highest healthy distance |\n",
    "\n",
    "## 8.4 Learned Baselines Example (FD002)\n",
    "\n",
    "```\n",
    "Cluster 0: 2,785 healthy samples\n",
    "  op_1 = 42.0, op_2 = 0.84 (high altitude, high Mach)\n",
    "  s_11: center=643.2, spread=4.1\n",
    "  s_12: center=521.8, spread=3.2\n",
    "\n",
    "Cluster 1: 1,645 healthy samples  \n",
    "  op_1 = 0.0, op_2 = 0.00 (sea level, ground idle)\n",
    "  s_11: center=518.7, spread=3.8\n",
    "  s_12: center=392.4, spread=2.9\n",
    "\n",
    "... (6 clusters total)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. C-MAPSS RUL Prediction Experiments\n",
    "\n",
    "## 9.1 Dataset Overview\n",
    "\n",
    "**C-MAPSS** (Commercial Modular Aero-Propulsion System Simulation) from NASA:\n",
    "\n",
    "| Subset | Training Engines | Test Engines | Operating Conditions | Fault Modes |\n",
    "|--------|-----------------|--------------|---------------------|-------------|\n",
    "| FD001 | 100 | 100 | 1 | 1 (HPC degradation) |\n",
    "| FD002 | 260 | 259 | 6 | 1 (HPC degradation) |\n",
    "| FD003 | 100 | 100 | 1 | 2 (HPC + Fan) |\n",
    "| FD004 | 248 | 249 | 6 | 2 (HPC + Fan) |\n",
    "\n",
    "### Sensors (21 total)\n",
    "\n",
    "```\n",
    "Near-constant (DROP): s1, s5, s6, s10, s16, s18, s19\n",
    "Informative (KEEP):   s2, s3, s4, s7, s8, s9, s11, s12, s13, s14, s15, s17, s20, s21\n",
    "```\n",
    "\n",
    "### Operating Conditions\n",
    "\n",
    "```\n",
    "op_1: Altitude (flight level indicator)       Range: 0-42\n",
    "op_2: Mach number                             Range: 0-0.84\n",
    "op_3: Throttle resolver angle (TRA)           Range: 100\n",
    "```\n",
    "\n",
    "### RUL Definition\n",
    "\n",
    "- **RUL** = Remaining Useful Life (cycles until failure)\n",
    "- **Capped at 125**: Values > 125 are clipped to 125 (early degradation is constant)\n",
    "- **Ground Truth**: `RUL_FD00X.txt` provides actual RUL at last cycle of each test engine\n",
    "\n",
    "## 9.2 State-of-the-Art Benchmarks\n",
    "\n",
    "Published SOTA results:\n",
    "\n",
    "| Method | FD001 RMSE | FD002 RMSE | FD003 RMSE | FD004 RMSE |\n",
    "|--------|------------|------------|------------|------------|\n",
    "| **SOTA (Deep Learning)** | 10.82 | 11.46 | 11.23 | 14.47 |\n",
    "| **SOTA (Traditional ML)** | 13.28 | 14.77 | 13.54 | 17.91 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 10. Best Model Configuration ⭐ UPDATED\n\n## 10.1 Latest Results (January 21, 2026)\n\n| Dataset | Best RMSE | Model | SOTA | Gap |\n|---------|-----------|-------|------|-----|\n| **FD001** | **13.10** | Stacking (LGB+GBR+XGB+Ridge) | 10.82 | +21.1% |\n| **FD002** | **14.15** | XGBoost | 11.46 | +23.5% |\n\n### Best by Dataset\n\n- **FD001**: Stacking ensemble (13.10) beats XGBoost alone (13.26) by 1.2%\n- **FD002**: XGBoost alone (14.15) is best; stacking slightly worse (14.22)\n\n## 10.2 Optimal Window Configuration\n\n```python\n# BEST: Extended windows capture both short and long-term dynamics\nWINDOWS = [5, 10, 20, 30, 50]\n\n# Previous (worse): Only medium-term\n# WINDOWS = [10, 20, 30]\n```\n\n**Why this works:**\n- Window 5: Captures rapid changes, immediate reactions\n- Window 50: Captures long-term degradation trends (MOST IMPORTANT)\n\n## 10.3 XGBoost Parameters\n\n```python\nXGB_PARAMS = {\n    'n_estimators': 500,        # Trees\n    'max_depth': 6,             # Tree depth\n    'learning_rate': 0.02,      # Learning rate\n    'subsample': 0.8,           # Row sampling\n    'colsample_bytree': 0.7,    # Feature sampling per tree\n    'min_child_weight': 2,      # Regularization\n    'reg_alpha': 0.1,           # L1 regularization\n    'reg_lambda': 1.0,          # L2 regularization\n    'random_state': 42,\n    'n_jobs': -1,\n}\n```\n\n## 10.4 Stacking Ensemble Configuration (Best for FD001)\n\n```python\nfrom sklearn.ensemble import StackingRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nstack = StackingRegressor(\n    estimators=[\n        ('lgb', LGBMRegressor(n_estimators=500, max_depth=6, learning_rate=0.02,\n                              subsample=0.8, colsample_bytree=0.7, random_state=42)),\n        ('gbr', GradientBoostingRegressor(n_estimators=300, max_depth=6,\n                                          learning_rate=0.05, subsample=0.8, random_state=42)),\n        ('xgb', XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.02,\n                             subsample=0.8, colsample_bytree=0.7, random_state=42)),\n    ],\n    final_estimator=Ridge(alpha=1.0),\n    cv=5,\n    n_jobs=-1,\n)\n```\n\n## 10.5 Top Features by Importance\n\n### FD001 (Single Operating Condition)\n\n| Rank | Feature | Importance | Description |\n|------|---------|------------|-------------|\n| 1 | `hd_slope_50` | 28.56% | Long-term degradation rate |\n| 2 | `hd_slope_30` | 26.63% | Medium-term degradation rate |\n| 3 | `hd_std_50` | 9.03% | Long-term volatility |\n| 4 | `hd_std_30` | 2.40% | Medium-term volatility |\n| 5 | `s_12_mean_50` | 1.43% | Sensor 12 long-term average |\n\n### FD002 (Six Operating Conditions)\n\n| Rank | Feature | Importance | Description |\n|------|---------|------------|-------------|\n| 1 | `hd_slope_50` | 30.55% | Long-term degradation rate |\n| 2 | `s_11_slope_50` | 13.81% | Sensor 11 long-term trend |\n| 3 | `hd_std_50` | 11.22% | Long-term volatility |\n| 4 | `hd_slope_30` | 9.13% | Medium-term degradation rate |\n| 5 | `s_11_mean_5` | 2.18% | Sensor 11 short-term average |\n\n**Key Insight**: `hd_slope_50` is the single most important feature in both datasets, capturing the long-term rate of degradation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Extended Windows Experiment ⭐ NEW\n",
    "\n",
    "## 11.1 Hypothesis\n",
    "\n",
    "Adding window sizes 5 (short-term) and 50 (long-term) to the standard [10, 20, 30] might capture additional dynamics:\n",
    "- **Window 5**: Rapid changes, immediate reactions to operating condition changes\n",
    "- **Window 50**: Sustained degradation trends over longer periods\n",
    "\n",
    "## 11.2 Experiment Design\n",
    "\n",
    "```python\n",
    "# Old configuration\n",
    "WINDOWS_OLD = [10, 20, 30]  # 3 windows\n",
    "\n",
    "# New configuration  \n",
    "WINDOWS_NEW = [5, 10, 20, 30, 50]  # 5 windows\n",
    "```\n",
    "\n",
    "## 11.3 Results\n",
    "\n",
    "| Dataset | Windows [10,20,30] | Windows [5,10,20,30,50] | Improvement |\n",
    "|---------|-------------------|------------------------|-------------|\n",
    "| FD001 | 13.83 RMSE | **13.26 RMSE** | **-4.1%** |\n",
    "| FD002 | 14.64 RMSE | **14.15 RMSE** | **-3.3%** |\n",
    "\n",
    "## 11.4 Feature Importance Analysis\n",
    "\n",
    "The extended windows revealed that **window 50 is crucial**:\n",
    "\n",
    "```\n",
    "Top 5 Features (FD002 with extended windows):\n",
    "\n",
    "1. hd_slope_50      30.55%   ← LONG-TERM degradation rate\n",
    "2. s_11_slope_50    13.81%   ← Sensor 11 LONG-TERM trend\n",
    "3. hd_std_50        11.22%   ← LONG-TERM volatility\n",
    "4. hd_slope_30       9.13%   ← Medium-term rate\n",
    "5. s_11_mean_5       2.18%   ← SHORT-TERM sensor level\n",
    "```\n",
    "\n",
    "**Conclusion**: Window 50 features dominate the top 3 positions. The model heavily relies on long-term trends to predict RUL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Feature Selection with XGBoost ⭐ NEW\n",
    "\n",
    "## 12.1 The Idea\n",
    "\n",
    "Let XGBoost tell you which features matter, then optionally retrain with only the top K features:\n",
    "\n",
    "```python\n",
    "# Train initial model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# Select top K features\n",
    "top_k = 50\n",
    "top_idx = np.argsort(importance)[-top_k:]\n",
    "selected_features = [feature_cols[i] for i in top_idx]\n",
    "\n",
    "# Retrain with selected features\n",
    "X_train_selected = X_train[:, top_idx]\n",
    "model_v2.fit(X_train_selected, y_train)\n",
    "```\n",
    "\n",
    "## 12.2 Experiment Results\n",
    "\n",
    "| Dataset | All Features (218) | Top 50 | Top 30 |\n",
    "|---------|-------------------|--------|--------|\n",
    "| FD001 | **13.26** | 13.67 | 13.82 |\n",
    "| FD002 | 14.15 | **14.11** | 14.16 |\n",
    "\n",
    "## 12.3 Findings\n",
    "\n",
    "1. **FD001**: All features performs best. Feature selection hurts slightly.\n",
    "2. **FD002**: Top 50 performs marginally better than all features.\n",
    "3. **Top 30 is too aggressive**: Loses important information.\n",
    "\n",
    "## 12.4 Recommendation\n",
    "\n",
    "- **Default**: Use all features (let XGBoost handle feature selection internally)\n",
    "- **If overfitting**: Try top 50 features\n",
    "- **For interpretability**: Analyze top 20 features to understand what drives predictions\n",
    "\n",
    "## 12.5 Top 20 Features (FD002)\n",
    "\n",
    "```\n",
    " 1. hd_slope_50        30.55%   [TREND]     Long-term degradation rate\n",
    " 2. s_11_slope_50      13.81%   [SENSOR]    Sensor 11 long-term trend\n",
    " 3. hd_std_50          11.22%   [VOLATILITY] Long-term variability\n",
    " 4. hd_slope_30         9.13%   [TREND]     Medium-term degradation rate\n",
    " 5. s_11_mean_5         2.18%   [SENSOR]    Sensor 11 short-term level\n",
    " 6. hd_std_30           2.06%   [VOLATILITY] Medium-term variability\n",
    " 7. s_15_slope_50       1.55%   [SENSOR]    Sensor 15 long-term trend\n",
    " 8. s_11_mean_10        1.02%   [SENSOR]    Sensor 11 short-term average\n",
    " 9. s_11_slope_30       0.91%   [SENSOR]    Sensor 11 medium-term trend\n",
    "10. hd_slope_20         0.82%   [TREND]     Short-term degradation rate\n",
    "...\n",
    "```\n",
    "\n",
    "**Pattern**: Trend features (slope) dominate, followed by volatility (std), then level (mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Ablation Study Results\n",
    "\n",
    "Systematic experiments comparing different feature configurations:\n",
    "\n",
    "## FD001 Results (Single Operating Condition)\n",
    "\n",
    "| Config | Description | RMSE | Gap to SOTA |\n",
    "|--------|-------------|------|-------------|\n",
    "| A | cycle only | 24.81 | +129% |\n",
    "| B | cycle + raw sensors | 16.84 | +56% |\n",
    "| C | cycle + healthy_distance (global) | 16.21 | +50% |\n",
    "| D | cycle + raw + HD | 15.73 | +45% |\n",
    "| E | D + regime ID | 15.68 | +45% |\n",
    "| F | D + temporal features (W=20) | 14.52 | +34% |\n",
    "| G | D + temporal (W=10,20,30) | 13.89 | +28% |\n",
    "| H | G + sensor slopes | 13.36 | +23% |\n",
    "| **I** | **H + extended windows (5,50)** | **13.26** | **+22.5%** |\n",
    "\n",
    "## FD002 Results (Six Operating Conditions)\n",
    "\n",
    "| Config | Description | RMSE | Gap to SOTA |\n",
    "|--------|-------------|------|-------------|\n",
    "| A | cycle only | 25.43 | +122% |\n",
    "| B | cycle + raw sensors | 17.10 | +49% |\n",
    "| C | cycle + HD (global baseline) | 18.94 | +65% |\n",
    "| C' | cycle + HD (per-regime baseline) | 16.52 | +44% |\n",
    "| D | cycle + raw + HD (per-regime) | 16.23 | +42% |\n",
    "| E | D + regime ID | 16.18 | +41% |\n",
    "| F | D + temporal features (W=20) | 15.67 | +37% |\n",
    "| G | D + temporal (W=10,20,30) | 15.34 | +34% |\n",
    "| H | G + sensor slopes | 15.04 | +31% |\n",
    "| **I** | **H + extended windows (5,50)** | **14.15** | **+23.5%** |\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "1. **Regime-aware baselines are critical for FD002**: Global baselines hurt performance (-21% worse than raw sensors)\n",
    "2. **Temporal features provide significant improvement**: +10-15% reduction in error\n",
    "3. **Multi-window features better than single window**: Captures both short and long-term trends\n",
    "4. **Extended windows [5, 50] add value**: Additional 3-4% improvement\n",
    "5. **Regime ID alone provides little value**: The regime-aware baselines capture most information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Regime-Aware Feature Engineering\n",
    "\n",
    "## 14.1 The Problem: Operating Condition Variation\n",
    "\n",
    "In multi-condition datasets (FD002, FD004), sensor values vary dramatically based on:\n",
    "- **Altitude** (op_1): Higher altitude = different thermal/pressure conditions\n",
    "- **Mach number** (op_2): Speed affects all aerodynamic sensors\n",
    "- **Throttle** (op_3): Power setting affects temperature sensors\n",
    "\n",
    "**Problem**: Raw sensor values from different operating conditions are not directly comparable.\n",
    "\n",
    "## 14.2 Solution: Per-Regime Healthy Baselines\n",
    "\n",
    "```python\n",
    "def compute_regime_baselines(train_df, n_regimes=6):\n",
    "    \"\"\"\n",
    "    For each operating regime:\n",
    "    1. Identify healthy portion (first 20% of life)\n",
    "    2. Compute mean and std for each sensor\n",
    "    3. Store as baseline for that regime\n",
    "    \"\"\"\n",
    "    # Step 1: Cluster operating conditions\n",
    "    kmeans = KMeans(n_clusters=n_regimes)\n",
    "    regime_labels = kmeans.fit_predict(train_df[['op_1', 'op_2']].values)\n",
    "    \n",
    "    # Step 2: For each regime, compute healthy baseline\n",
    "    baselines = {}\n",
    "    for regime in range(n_regimes):\n",
    "        regime_data = train_df[regime_labels == regime]\n",
    "        healthy_data = regime_data[regime_data['life_pct'] < 0.20]\n",
    "        \n",
    "        baselines[regime] = {}\n",
    "        for sensor in sensor_cols:\n",
    "            values = healthy_data[sensor]\n",
    "            baselines[regime][sensor] = {\n",
    "                'mean': values.mean(),\n",
    "                'std': values.std() + 1e-10\n",
    "            }\n",
    "    \n",
    "    return baselines\n",
    "```\n",
    "\n",
    "## 14.3 Healthy Distance Computation\n",
    "\n",
    "```python\n",
    "def compute_healthy_distance(row, regime, baselines):\n",
    "    \"\"\"\n",
    "    Compute z-score distance from regime-specific baseline.\n",
    "    \"\"\"\n",
    "    distances = {}\n",
    "    baseline = baselines[regime]\n",
    "    \n",
    "    for sensor in sensor_cols:\n",
    "        value = row[sensor]\n",
    "        mean = baseline[sensor]['mean']\n",
    "        std = baseline[sensor]['std']\n",
    "        \n",
    "        # Z-score: how many std from healthy mean?\n",
    "        distances[f'hd_{sensor}'] = abs(value - mean) / std\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    hd_values = list(distances.values())\n",
    "    distances['hd_mean'] = np.mean(hd_values)\n",
    "    distances['hd_max'] = np.max(hd_values)\n",
    "    distances['hd_std'] = np.std(hd_values)\n",
    "    \n",
    "    return distances\n",
    "```\n",
    "\n",
    "## 14.4 Temporal Features on Healthy Distance\n",
    "\n",
    "```python\n",
    "def compute_temporal_features(hd_history, windows=[5, 10, 20, 30, 50]):\n",
    "    \"\"\"\n",
    "    Compute rolling window features on healthy distance.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    n = len(hd_history)\n",
    "    \n",
    "    for W in windows:\n",
    "        if n >= W:\n",
    "            recent = hd_history[-W:]\n",
    "            \n",
    "            # Slope: Linear fit over window\n",
    "            x = np.arange(W)\n",
    "            slope = np.polyfit(x, recent, 1)[0]\n",
    "            features[f'hd_slope_{W}'] = slope\n",
    "            \n",
    "            # Delta: Change from W cycles ago\n",
    "            features[f'hd_delta_{W}'] = hd_history[-1] - hd_history[-W]\n",
    "            \n",
    "            # Volatility: Standard deviation over window\n",
    "            features[f'hd_std_{W}'] = np.std(recent)\n",
    "            \n",
    "            # Curvature: Acceleration\n",
    "            if W >= 10:\n",
    "                mid = W // 2\n",
    "                slope1 = (recent[mid] - recent[0]) / mid\n",
    "                slope2 = (recent[-1] - recent[mid]) / (W - mid)\n",
    "                features[f'hd_curv_{W}'] = slope2 - slope1\n",
    "    \n",
    "    return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Complete Code Reference\n",
    "\n",
    "## 15.1 Feature Engineering Pipeline (Full Code)\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete Feature Engineering Pipeline for C-MAPSS\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Configuration\n",
    "COLS = ['unit', 'cycle', 'op_1', 'op_2', 'op_3'] + [f's_{i}' for i in range(1, 22)]\n",
    "SIGNAL_COLS = ['s_2', 's_3', 's_4', 's_7', 's_8', 's_9', 's_11', 's_12',\n",
    "               's_13', 's_14', 's_15', 's_17', 's_20', 's_21']\n",
    "WINDOWS = [5, 10, 20, 30, 50]  # Extended windows\n",
    "\n",
    "\n",
    "def load_cmapss(data_dir, subset):\n",
    "    \"\"\"Load C-MAPSS data.\"\"\"\n",
    "    train_df = pd.read_csv(data_dir / f'train_{subset}.txt', sep=r'\\s+', \n",
    "                           header=None, names=COLS)\n",
    "    test_df = pd.read_csv(data_dir / f'test_{subset}.txt', sep=r'\\s+', \n",
    "                          header=None, names=COLS)\n",
    "    \n",
    "    with open(data_dir / f'RUL_{subset}.txt', 'r') as f:\n",
    "        rul_true = np.array([float(line.strip()) for line in f if line.strip()])\n",
    "    \n",
    "    # Add RUL to training data (capped at 125)\n",
    "    max_cycles = train_df.groupby('unit')['cycle'].max().rename('max_cycle')\n",
    "    train_df = train_df.merge(max_cycles, on='unit')\n",
    "    train_df['RUL'] = (train_df['max_cycle'] - train_df['cycle']).clip(upper=125)\n",
    "    train_df = train_df.drop(columns=['max_cycle'])\n",
    "    \n",
    "    return train_df, test_df, rul_true\n",
    "\n",
    "\n",
    "class ClusterNormalizer:\n",
    "    \"\"\"Normalize signals by operating regime.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=6, healthy_pct=0.20):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.healthy_pct = healthy_pct\n",
    "        self.kmeans = None\n",
    "        self.scaler = None\n",
    "        self.baselines = {}\n",
    "    \n",
    "    def fit(self, df, op_cols, signal_cols, entity_col, time_col):\n",
    "        # Cluster operating conditions\n",
    "        op_data = df[op_cols].values\n",
    "        self.scaler = StandardScaler()\n",
    "        op_scaled = self.scaler.fit_transform(op_data)\n",
    "        \n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
    "        labels = self.kmeans.fit_predict(op_scaled)\n",
    "        \n",
    "        df = df.copy()\n",
    "        df['_cluster'] = labels\n",
    "        df['_life_pct'] = df.groupby(entity_col)[time_col].transform(\n",
    "            lambda x: (x - x.min()) / (x.max() - x.min() + 1e-10)\n",
    "        )\n",
    "        \n",
    "        # Compute healthy baselines\n",
    "        healthy = df[df['_life_pct'] <= self.healthy_pct]\n",
    "        \n",
    "        for cluster in range(self.n_clusters):\n",
    "            cluster_healthy = healthy[healthy['_cluster'] == cluster]\n",
    "            self.baselines[cluster] = {}\n",
    "            \n",
    "            for signal in signal_cols:\n",
    "                values = cluster_healthy[signal].dropna()\n",
    "                if len(values) > 0:\n",
    "                    self.baselines[cluster][signal] = {\n",
    "                        'mean': values.mean(),\n",
    "                        'std': values.std() + 1e-10\n",
    "                    }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df, op_cols, signal_cols):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Assign clusters\n",
    "        op_scaled = self.scaler.transform(df[op_cols].values)\n",
    "        df['cluster_id'] = self.kmeans.predict(op_scaled)\n",
    "        \n",
    "        # Normalize signals\n",
    "        for signal in signal_cols:\n",
    "            normalized = np.zeros(len(df))\n",
    "            \n",
    "            for cluster in range(self.n_clusters):\n",
    "                mask = df['cluster_id'] == cluster\n",
    "                if cluster in self.baselines and signal in self.baselines[cluster]:\n",
    "                    baseline = self.baselines[cluster][signal]\n",
    "                    values = df.loc[mask, signal].values\n",
    "                    normalized[mask] = np.abs(values - baseline['mean']) / baseline['std']\n",
    "            \n",
    "            df[f'{signal}_norm'] = normalized\n",
    "        \n",
    "        # Aggregate healthy distance\n",
    "        norm_cols = [f'{s}_norm' for s in signal_cols]\n",
    "        df['hd_mean'] = df[norm_cols].mean(axis=1)\n",
    "        df['hd_max'] = df[norm_cols].max(axis=1)\n",
    "        df['hd_std'] = df[norm_cols].std(axis=1)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "def compute_rolling_features(df, entity_col, time_col, signal_col, windows):\n",
    "    \"\"\"Compute rolling window features per entity.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for entity in df[entity_col].unique():\n",
    "        entity_df = df[df[entity_col] == entity].sort_values(time_col).copy()\n",
    "        values = entity_df[signal_col].values\n",
    "        n = len(values)\n",
    "        \n",
    "        for W in windows:\n",
    "            # Initialize\n",
    "            entity_df[f'{signal_col}_mean_{W}'] = np.nan\n",
    "            entity_df[f'{signal_col}_std_{W}'] = np.nan\n",
    "            entity_df[f'{signal_col}_slope_{W}'] = np.nan\n",
    "            entity_df[f'{signal_col}_delta_{W}'] = np.nan\n",
    "            \n",
    "            for i in range(W - 1, n):\n",
    "                window = values[i - W + 1:i + 1]\n",
    "                idx = entity_df.index[i]\n",
    "                \n",
    "                entity_df.loc[idx, f'{signal_col}_mean_{W}'] = np.mean(window)\n",
    "                entity_df.loc[idx, f'{signal_col}_std_{W}'] = np.std(window)\n",
    "                entity_df.loc[idx, f'{signal_col}_delta_{W}'] = values[i] - values[i - W + 1]\n",
    "                \n",
    "                # Slope\n",
    "                x = np.arange(W)\n",
    "                entity_df.loc[idx, f'{signal_col}_slope_{W}'] = np.polyfit(x, window, 1)[0]\n",
    "        \n",
    "        results.append(entity_df)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "def run_pipeline(data_dir, subset, n_clusters):\n",
    "    \"\"\"Run complete pipeline.\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    train_df, test_df, rul_true = load_cmapss(data_dir, subset)\n",
    "    \n",
    "    # Cluster normalization\n",
    "    normalizer = ClusterNormalizer(n_clusters=n_clusters)\n",
    "    normalizer.fit(train_df, ['op_1', 'op_2'], SIGNAL_COLS, 'unit', 'cycle')\n",
    "    \n",
    "    train_norm = normalizer.transform(train_df, ['op_1', 'op_2'], SIGNAL_COLS)\n",
    "    test_norm = normalizer.transform(test_df, ['op_1', 'op_2'], SIGNAL_COLS)\n",
    "    \n",
    "    # Rolling features on hd_mean\n",
    "    train_feat = compute_rolling_features(train_norm, 'unit', 'cycle', 'hd_mean', WINDOWS)\n",
    "    test_feat = compute_rolling_features(test_norm, 'unit', 'cycle', 'hd_mean', WINDOWS)\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [c for c in train_feat.columns \n",
    "                    if c not in COLS + ['RUL', 'unit', 'cluster_id']]\n",
    "    feature_cols = [c for c in feature_cols if train_feat[c].notna().sum() > 0.5 * len(train_feat)]\n",
    "    \n",
    "    # Train XGBoost\n",
    "    X_train = train_feat[feature_cols].fillna(0).values\n",
    "    y_train = train_feat['RUL'].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model = XGBRegressor(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.02,\n",
    "        subsample=0.8, colsample_bytree=0.7, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_s, y_train)\n",
    "    \n",
    "    # Predict on test (last cycle per unit)\n",
    "    test_last = test_feat.groupby('unit').last().reset_index()\n",
    "    for col in feature_cols:\n",
    "        if col not in test_last.columns:\n",
    "            test_last[col] = 0\n",
    "    \n",
    "    X_test = test_last[feature_cols].fillna(0).values\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "    \n",
    "    y_pred = np.clip(model.predict(X_test_s), 0, 125)\n",
    "    rul_capped = np.clip(rul_true, 0, 125)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(rul_capped, y_pred))\n",
    "    \n",
    "    return rmse, model, feature_cols\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = Path('data')\n",
    "    \n",
    "    # FD001: 1 operating condition\n",
    "    rmse, _, _ = run_pipeline(data_dir, 'FD001', n_clusters=1)\n",
    "    print(f\"FD001 RMSE: {rmse:.2f}\")\n",
    "    \n",
    "    # FD002: 6 operating conditions\n",
    "    rmse, _, _ = run_pipeline(data_dir, 'FD002', n_clusters=6)\n",
    "    print(f\"FD002 RMSE: {rmse:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Using orthon.features Module\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Using the orthon.features module for feature engineering.\n",
    "\"\"\"\n",
    "\n",
    "from orthon.features import (\n",
    "    RollingConfig,\n",
    "    RollingFeatureEngine,\n",
    "    ClusterNormalizer,\n",
    "    FeatureEngineeringPipeline,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load your data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Option 1: Full Pipeline (recommended)\n",
    "pipeline = FeatureEngineeringPipeline(\n",
    "    n_clusters=6,\n",
    "    windows=[5, 10, 20, 30, 50],\n",
    "    op_cols=['op_1', 'op_2'],\n",
    "    signal_cols=['s_11', 's_12', 's_15'],\n",
    "    healthy_pct=0.20,\n",
    ")\n",
    "\n",
    "train_feat = pipeline.fit_transform(train_df, entity_col='unit', time_col='cycle')\n",
    "test_feat = pipeline.transform(test_df)\n",
    "\n",
    "# Option 2: Separate Steps\n",
    "# Step 2a: Cluster normalization\n",
    "normalizer = ClusterNormalizer(n_clusters=6, healthy_pct=0.20)\n",
    "normalizer.fit(train_df, op_cols=['op_1', 'op_2'], signal_cols=['s_11', 's_12'],\n",
    "               entity_col='unit', time_col='cycle')\n",
    "train_norm = normalizer.compute_healthy_distance(train_df)\n",
    "\n",
    "# Step 2b: Rolling features\n",
    "config = RollingConfig(\n",
    "    windows=[5, 10, 20, 30, 50],\n",
    "    compute_slope=True,\n",
    "    compute_delta=True,\n",
    "    compute_std=True,\n",
    ")\n",
    "engine = RollingFeatureEngine(config=config)\n",
    "train_rolling = engine.compute_multi_signal(\n",
    "    data=train_norm,\n",
    "    signal_cols=['hd_mean'],\n",
    "    entity_col='unit',\n",
    "    sort_col='cycle',\n",
    ")\n",
    "\n",
    "# Train model\n",
    "feature_cols = [c for c in train_feat.columns if c.startswith('hd_') or c.endswith('_norm')]\n",
    "X_train = train_feat[feature_cols].fillna(0).values\n",
    "y_train = train_feat['RUL'].values\n",
    "\n",
    "model = XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.02)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importance = model.feature_importances_\n",
    "top_10 = np.argsort(importance)[-10:][::-1]\n",
    "\n",
    "print(\"Top 10 Features:\")\n",
    "for i, idx in enumerate(top_10):\n",
    "    print(f\"  {i+1}. {feature_cols[idx]}: {importance[idx]:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 16. Experiment Log ⭐ UPDATED\n\n## Chronological Record of All Experiments\n\n| Date | Experiment | FD001 | FD002 | Notes |\n|------|------------|-------|-------|-------|\n| Jan 20 | Baseline (raw sensors) | 16.84 | 17.10 | No feature engineering |\n| Jan 20 | + Healthy Distance (global) | 16.21 | 18.94 | Global baseline HURTS FD002 |\n| Jan 20 | + HD (per-regime) | 15.73 | 16.23 | Per-regime is critical |\n| Jan 20 | + Rolling W=[10,20,30] | 13.89 | 15.34 | Temporal features help |\n| Jan 20 | + Sensor slopes | 13.36 | 15.04 | Previous best |\n| Jan 20 | + Extended W=[5,10,20,30,50] | 13.26 | 14.15 | XGBoost best |\n| Jan 20 | Extended + Top 50 features | 13.67 | 14.11 | Slight degradation |\n| Jan 20 | Extended + Top 30 features | 13.82 | 14.16 | Too aggressive |\n| Jan 21 | **Stacking Ensemble** | **13.10** | 14.22 | **NEW BEST for FD001** |\n\n## Individual Model Comparison (Stacking Experiment)\n\n| Model | FD001 RMSE | FD002 RMSE |\n|-------|------------|------------|\n| LightGBM | 13.29 | 14.29 |\n| GradientBoosting | 13.83 | 14.40 |\n| XGBoost | 13.26 | 14.15 |\n| **Stacking (LGB+GBR+XGB+Ridge)** | **13.10** | 14.22 |\n\n## Key Learnings\n\n1. **Regime-aware normalization is essential** for multi-condition datasets\n2. **Window 50 is critical** - captures long-term degradation trends\n3. **Window 5 adds value** - captures rapid short-term changes\n4. **Keep all features** - XGBoost handles feature selection internally\n5. **hd_slope_50 is #1 feature** - long-term degradation rate dominates\n6. **Stacking helps on FD001** - ensemble diversity improves single-condition predictions\n7. **XGBoost alone best for FD002** - stacking adds slight noise on multi-condition data\n\n## Files Created During Experiments\n\n| File | Description |\n|------|-------------|\n| `proof/best_rul_model.py` | Best model configuration |\n| `proof/fd002_regime_test.py` | Standalone regime test |\n| `proof/test_feature_pipeline_cmapss.py` | Full pipeline test |\n| `proof/test_extended_windows.py` | Extended windows experiment |\n| `proof/test_stacking_ensemble.py` | Stacking ensemble experiment |\n| `orthon/features/rolling_features.py` | Feature engineering module |\n| `examples/feature_engineering_demo.py` | Demo script |"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Summary\n\n## What We Built\n\n1. **orthon package** (v0.1.0): Pip-installable behavioral geometry engine with:\n   - 11 vector engines (Hurst, entropy, GARCH, wavelets, etc.)\n   - 9 geometry engines (PCA, clustering, MST, etc.)\n   - 7 state engines (Granger, DTW, DMD, etc.)\n   - 5 temporal dynamics engines\n   - 3 observation-level engines\n   - **Feature engineering module** (rolling stats, cluster normalization)\n\n2. **C-MAPSS RUL Prediction Pipeline**:\n   - Cluster normalization (regime-aware baselines)\n   - Extended rolling windows [5, 10, 20, 30, 50]\n   - Stacking ensemble (LGB + GBR + XGB + Ridge)\n   - **Best results: FD001 13.10 RMSE, FD002 14.15 RMSE**\n\n3. **Complete Documentation**:\n   - API reference for all engines\n   - Rolling window statistics reference\n   - Cluster normalization guide\n   - Ablation study results\n   - Stacking ensemble analysis\n   - Experiment log\n   - Reproducible code scripts\n\n## Best Configuration by Dataset\n\n### FD001 (Single Operating Condition)\n```python\n# Use stacking ensemble\nmodel = StackingRegressor(\n    estimators=[\n        ('lgb', LGBMRegressor(...)),\n        ('gbr', GradientBoostingRegressor(...)),\n        ('xgb', XGBRegressor(...)),\n    ],\n    final_estimator=Ridge(alpha=1.0),\n)\n# Result: 13.10 RMSE (gap to SOTA: +21.1%)\n```\n\n### FD002 (Six Operating Conditions)\n```python\n# Use XGBoost alone\nmodel = XGBRegressor(\n    n_estimators=500, max_depth=6, learning_rate=0.02,\n    subsample=0.8, colsample_bytree=0.7,\n)\n# Result: 14.15 RMSE (gap to SOTA: +23.5%)\n```\n\n### Common Configuration\n```python\nWINDOWS = [5, 10, 20, 30, 50]  # Extended windows\nN_CLUSTERS = {'FD001': 1, 'FD002': 6}\nHEALTHY_PCT = 0.20\n```\n\n## Key Insights\n\n1. **Regime-aware baselines are essential** for multi-condition datasets\n2. **Window 50 captures long-term degradation** - most important feature\n3. **Window 5 captures rapid changes** - complements long-term trends\n4. **hd_slope_50 is #1 feature** - 30% of model importance\n5. **Keep all features** - XGBoost handles selection internally\n6. **Stacking helps single-condition datasets** - FD001 improved 1.2%\n7. **XGBoost alone best for multi-condition** - FD002 doesn't benefit from stacking\n\n## Results Summary\n\n| Dataset | Best Model | RMSE | SOTA | Gap |\n|---------|------------|------|------|-----|\n| FD001 | Stacking | **13.10** | 10.82 | +21.1% |\n| FD002 | XGBoost | **14.15** | 11.46 | +23.5% |\n\n---\n\n**Package Repository:** https://github.com/prism-engines/diagnostics  \n**Version:** 0.1.0  \n**License:** MIT  \n**Last Updated:** January 21, 2026",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Built\n",
    "\n",
    "1. **orthon package** (v0.1.0): Pip-installable behavioral geometry engine with:\n",
    "   - 11 vector engines (Hurst, entropy, GARCH, wavelets, etc.)\n",
    "   - 9 geometry engines (PCA, clustering, MST, etc.)\n",
    "   - 7 state engines (Granger, DTW, DMD, etc.)\n",
    "   - 5 temporal dynamics engines\n",
    "   - 3 observation-level engines\n",
    "   - **Feature engineering module** (rolling stats, cluster normalization)\n",
    "\n",
    "2. **C-MAPSS RUL Prediction Pipeline**:\n",
    "   - Cluster normalization (regime-aware baselines)\n",
    "   - Extended rolling windows [5, 10, 20, 30, 50]\n",
    "   - **Best results: FD001 13.26 RMSE, FD002 14.15 RMSE**\n",
    "\n",
    "3. **Complete Documentation**:\n",
    "   - API reference for all engines\n",
    "   - Rolling window statistics reference\n",
    "   - Cluster normalization guide\n",
    "   - Ablation study results\n",
    "   - Experiment log\n",
    "   - Reproducible code scripts\n",
    "\n",
    "## Best Configuration\n",
    "\n",
    "```python\n",
    "WINDOWS = [5, 10, 20, 30, 50]  # Extended windows\n",
    "N_CLUSTERS = {'FD001': 1, 'FD002': 6}\n",
    "HEALTHY_PCT = 0.20\n",
    "\n",
    "XGB_PARAMS = {\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.02,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "}\n",
    "```\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "1. **Regime-aware baselines are essential** for multi-condition datasets\n",
    "2. **Window 50 captures long-term degradation** - most important feature\n",
    "3. **Window 5 captures rapid changes** - complements long-term trends\n",
    "4. **hd_slope_50 is #1 feature** - 30% of model importance\n",
    "5. **Keep all features** - XGBoost handles selection internally\n",
    "\n",
    "---\n",
    "\n",
    "**Package Repository:** https://github.com/prism-engines/diagnostics  \n",
    "**Version:** 0.1.0  \n",
    "**License:** MIT  \n",
    "**Last Updated:** January 20, 2026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}